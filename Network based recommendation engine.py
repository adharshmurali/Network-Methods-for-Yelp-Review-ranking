# -*- coding: utf-8 -*-
"""SI_608_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EdvbrxVildjtTgLWv_2mM4In8EuWFWXH
"""

# Import libraries
import pandas as pd
import numpy as np
import networkx as nx

# Mount to your own google drive
from google.colab import drive
drive.mount('/content/gdrive')

# Check files
# 1. To access files, you need to copy the folder onto your own gdrive. 
# 2. Each of you need to mount your own gdrive. 
# 3. The path should match the one below exactly or the code will not run for everyone else.
import os
os.listdir('gdrive/My Drive/SI_608_Group_Project/yelp_dataset')

"""# Koki's Code Section"""

# Load Files
yelp_buisness = pd.read_json('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/yelp_academic_dataset_business.json', lines=True)

yelp_buisness.head(5)

# print number of rows or buisnesses
yelp_buisness.shape[0]

# All columns
buisness_cols = list(yelp_buisness.columns)

print("The yelp buisness dataset contains the following columns:")
buisness_cols

# Detailed memory usage information about buisness dataframe
yelp_buisness.info(memory_usage='deep')

# Filter out non-relevant columns
yelp_buisness = yelp_buisness.drop(['address', 'hours', 'latitude', 'longitude', 'neighborhood', 'postal_code'], axis=1)

yelp_buisness.info(memory_usage='deep')

# What establishments are open? closed? 
yelp_buisness['is_open'].value_counts()

# Filter out establishments that are not open
yelp_buisness = yelp_buisness[yelp_buisness['is_open'] == 1]

# Check that closed establishments are removed 
yelp_buisness['is_open'].unique()

# How many states are there? 
print('There are a total of {} states'.format(len(yelp_buisness['state'].unique())))
yelp_buisness['state'].unique()

"""## Some of these are non-valid state codes. Not quite sure what the meaning is for these unknown labels. For now, keep them with the data until we find some documentation explaining them"""

yelp_buisness['state'].value_counts()

"""## It seems the data is mostly gathered from Arizona. Odd, since considering that cities such as Los Angels has many resturants and establishments, yet it is not included within this dataset

## Run the code below if you want to remove all non-valid state codes
"""

# Taken from https://gist.github.com/JeffPaine/3083347
valid_state_codes = [
          "AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DC", "DE", "FL", "GA", 
          "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", 
          "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", 
          "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", 
          "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"
          ]

# Drop non state codes
yelp_buisness = yelp_buisness[yelp_buisness['state'].isin(valid_state_codes)]

# Check 
yelp_buisness['state'].unique()

"""## Find all buisnesses that either have 'Food' or 'Resturants' label in their categories"""

# Show all categories 
yelp_buisness.groupby('categories')['categories'].count()[:10]

import re

# Convert category entries from string to set and keep only unique values

# Initialize empty set
unique_terms = set()

# Regex to capture all category labels
p = re.compile('\,\s')

counter = 0 
# Iterate through every row of categories
for x in yelp_buisness['categories']: 
  if x:
    # Convert all words to lowercase 
    unique_terms.update(p.split(x))

# Check
print("Total number of unique labels: {}".format(len(unique_terms)))
print(unique_terms)

# Find all resturants
# 1. food - 1920 matches
# 2. Food - 27015 matches
# 3. Food|food - 28443 matches
# 4. Restaurants - 41342 matches
# 5. restaurants - 0 matches 
# 6. Food|Restaurants|food - 53793 matches 
# 7. Food|Restaurants - 53793 matches (same number so omit food)

# Some mistakes captured such as: 
# Health & Medical, Active Life, Vitamins & Supplements, Nutritionists, Food, Trainers, Specialty Food, Fitness & Instruction, Health Markets, Shopping

restaurant_reviews = yelp_buisness[yelp_buisness['categories'].str.contains('Food|Restaurants')==True]

# Check
print("Number of relevant reviews: {}".format(len(restaurant_reviews)))

# View 
restaurant_reviews.head(10)

# TESTING
# counts = 0
# for x in restaurant_reviews['categories']:
#   if 'Health & Medical' in x:
#     counts += 1 
#     print(x)

# print(counts)

# Find all restaurants
# 1. food - 1920 matches
# 2. Food - 27015 matches
# 3. Food|food - 28443 matches
# 4. Restaurants - 41342 matches
# 5. restaurants - 0 matches 
# 6. Food|Restaurants|food - 53793 matches 
# 7. Food|Restaurants - 53793 matches (same number so omit food)

# Some mistakes captured such as: 
# Health & Medical, Active Life, Vitamins & Supplements, Nutritionists, Food, Trainers, Specialty Food, Fitness & Instruction, Health Markets, Shopping
# Food is contained here, but it is a drug store and not an pure restaurant or food place 
restaurant_buisnesses = yelp_buisness[yelp_buisness['categories'].str.contains('Restaurants')==True]

# Check
print("Number of relevant buisnesses: {}".format(len(restaurant_buisnesses)))

# View 
restaurant_buisnesses.head(10)

# Initialize empty set
unique_terms = set()

# Regex to capture all category labels
p = re.compile('\,\s')

counter = 0 
# Iterate through every row of categories
for x in restaurant_buisnesses['categories']: 
    if x:
        # Convert all words to lowercase 
        unique_terms.update(p.split(x))

# Check
print("Total number of unique labels: {}".format(len(unique_terms)))
print(unique_terms)

# Manually filter out irrelevant tags 
# For example, the following buisness contains the word 'Food' in their category
# However, it is a nutrition store and not a place people usually go to for food or drinks, 
restaurant_buisnesses[restaurant_buisnesses['categories'].str.contains('Property Management')==True][['categories', 'name']]

restaurant_buisnesses.info(memory_usage='deep')

resturant_ID = restaurant_buisnesses['business_id']

resturant_ID.to_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/restaurant_ID_v3.csv', index=False)

restaurant_reviews['categories'].head(10)

"""# Adharsh Code Section"""

##Loading in the reviews_dataset
data=[]
review_data=pd.read_json('gdrive/My Drive/yelp_dataset/yelp_academic_dataset_review.json',lines=True,chunksize=100000)

review_data

for i in review_data:
  data.append(i[['user_id','stars','business_id']])

review_dataset=pd.concat(data)

#Review_dataset with all
review_dataset.head()

#Loading in the business id of the restaurant
restaurantid=pd.read_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/restaurant_ID_v2.csv',header=None)

restaurantid.columns

# restaurantid['Apn5Q_b6Nz61Tq4XzPdf9A'].head(5)

review_dataset=review_dataset[review_dataset['business_id'].isin(restaurantid[0])]

review_dataset.to_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/reviews.csv')

review_dataset.shape

len((review_dataset['business_id'].unique()))

#Just interesting facts is that there are people who give more reviews than excepted.
plot_data = review_dataset.groupby('business_id')['business_id'].count().sort_values(ascending=False)

from matplotlib import pyplot as plt
# %matplotlib inline
import seaborn as sns

# Set the style of seaborn plot
sns.set(style='darkgrid', palette='husl', font_scale=1.5)

# Create matplotlib Figure and Axes object
f, ax = plt.subplots(figsize=(15,9))

# Create the plot
g = sns.distplot(plot_data)

# Fine Tuning font size
g.set_xlabel('Number of reviews', fontsize=20)
# g.set_ylabel('Number of Nodes', fontsize=20)
g.axes.set_title('Distribution of reviews per buisness', fontsize=30)

# Display
plt.show(g)

buisnesses_greater_than_10 = [x for x in plot_data if x >= 5]
buisnesses_greater_than_10[:5]

# Number of valid businesses
len(buisnesses_greater_than_10)

# Set the style of seaborn plot
sns.set(style='darkgrid', palette='husl', font_scale=1.5)

# Create matplotlib Figure and Axes object
f, ax = plt.subplots(figsize=(15,9))

# Create the plot
g = sns.distplot(buisnesses_greater_than_10)

# Fine Tuning font size
g.set_xlabel('Number of reviews', fontsize=20)
# g.set_ylabel('Number of Nodes', fontsize=20)
g.axes.set_title('Distribution of reviews per buisness', fontsize=30)

# Display
plt.show(g)



#Just looking at the number of stars given by the user
review_dataset[review_dataset['user_id']=='CxDOIDnH8gp9KXzpBHJYXw']
#Exploring this user and finding if it is possible to understand their cusine interest would be a great starting point.
#It would be ideal to throw people who has given only review as it would weaken our algorithm.

#In all there are 17 million reviews in the dataset
count=0
for i in review_data:
  count=count+1
print(count)

#Loading the json reader together into a data list
for i in range(0,10):
  data.append(review_data[i])
#Concatenating the chunks together to load the entire dataset
review_dataset=pd.DataFrame(data)

import pandas as pd
import os
import numpy as np
import json
import networkx as nx

os.listdir('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/')

##Loading in the user_dataset
data=[]
yelp_user=pd.read_json('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/yelp_academic_dataset_user.json',lines=True,chunksize=100000)

#Loading the json reader together into a data list
for i in yelp_user:
  data.append(i)
#Concatenating the chunks together to load the entire dataset
yelp_user=pd.concat(data)

yelp_user.sample(5)

print('The yelp_user data includes the following columns: \n', yelp_user.columns.tolist())

# Basic description for yelp_user dataset
yelp_user.describe()

# Average number of friends for users with >0 friends
np.average(yelp_user['friends'].replace('None',np.nan).dropna().str.len())

# Creating undirected graph with 'user_id' as nodes and 'friends' as edges
G_users=nx.convert_matrix.from_pandas_edgelist(yelp_user,'user_id','friends',edge_attr=None, create_using=nx.Graph())

G_users.number_of_nodes()

np.s(dict(G_users.degree()).values())

import networkx as nx
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from math import sqrt
import statistics
import math
import random
import numpy as np
from statistics import mode

"""Build User_Business BiPartite"""

train = pd.read_csv('train.csv')[['user_id','stars','business_id']]
test = pd.read_csv('test.csv')[['user_id','stars','business_id']]
average_rating = train.groupby('business_id').mean().reset_index().rename(columns={'stars':'business_average_rating'})
# test_labels = test.stars
# test = test[['user_id','business_id']]

train['graph_tuple'] = list(zip(train.user_id, train.business_id, train.stars))

train_user_ids = train.user_id.tolist()
train_business_ids = train.business_id.tolist()
test['user_business'] = list(zip(test.user_id, test.business_id))

B=nx.Graph()

bipart_0 = list(train.user_id)
bipart_1 = list(train.business_id)

B.add_nodes_from(bipart_0, bipartite=0)
B.add_nodes_from(bipart_1, bipartite=1)

B.add_weighted_edges_from(train.graph_tuple, weight='avg rating')

"""Test_Train Split"""

from sklearn.model_selection import StratifiedShuffleSplit
data = pd.read_csv('reviews_filtered.csv')[['user_id', 'business_id','stars']]

"""We want to make sure that a user in the test set has at least one instance in the train set. This is where we filter out any user that has only one review, and then create a .75/.25 split. However, when we come across a new user_id, we always add it into the train set first. One further instances of the user_id, we randomly assign to either test or train"""

grouped_users = data.groupby('user_id')['business_id'].count().reset_index(name = 'counts')
grouped_users = grouped_users[grouped_users.counts > 2].user_id.tolist()

data = data[data.user_id.isin(grouped_users)]

"""These next two cells allow us to properly create a test train split"""

test_list = []
user_list = []
train_list = []

for index, row in data.iterrows():
    if row.user_id not in user_list:
        train_list.append(index)
        user_list.append(row.user_id)
    else:
        rand_num = random.randint(0,4)
        if rand_num == 0:
            test_list.append(index)
        else:
            train_list.append(index)

train, test = data.loc[train_list],data.loc[test_list]
train_user, train_business = train.user_id.tolist(), train.business_id.tolist()

"""These list comprehensions see that there are no users or businesses in the test data that are not in the training data"""

[i for i in test.user_id if i not in train_user],[i for i in test.business_id if i not in train_business]

test = test[~test.business_id.isin([i for i in test.business_id if i not in train_business])]

train.to_csv('equal_distribution_train.csv', index=False)
test.to_csv('equal_distribution_test.csv', index=False)

"""Create user_category BiPartite"""

yelp_buisness = pd.read_json('yelp_academic_dataset_business.json', lines=True)
restaurant_reviews = yelp_buisness[yelp_buisness['categories'].str.contains('Restaurants')==True]
restaurant_reviews = restaurant_reviews[['business_id', 'categories', 'city', 'name','state']]
# df_reviews = pd.read_csv('reviews_2.csv')
X_train, y_train = pd.read_csv('X_train.csv')[['user_id', 'business_id']], pd.read_csv('Y_train.csv',header=None)[1]
X_train['stars'] = y_train
df_reviews = X_train

merged = df_reviews.merge(restaurant_reviews, left_on='business_id', right_on='business_id')

# categories = ['American', 'Italian', 'Chinese', 'Japanese', 'Indian', 'Thai', 'Mexican', 'Korean', 'Pizza',\
#               'Middle Eastern', 'Mediterranean', 'Greek', 'French']

categories = ['American', 'Italian', 'Chinese', 'Indian', 'Mexican']

categories = ['American', 'Italian', 'Indian', 'Mexican']

for i in categories:
    merged.loc[merged.categories.str.contains(i), i] = merged.stars
    merged.loc[~merged.categories.str.contains(i), i] = np.nan

matrix = merged.groupby('user_id').mean()

matrix = matrix[[i for i in matrix.columns if i != 'stars']]

matrix['user_id'] = matrix.index

for col in [i for i in matrix.columns if i != 'user_id']:
    matrix['user_{}_tuple'.format(col)] = matrix.apply(lambda x: [x.user_id,col,x[col]], axis = 1)

matrix = matrix[[i for i in matrix.columns if i.endswith('tuple') == True]]

bipart_0 = list(matrix.columns.tolist())
bipart_1 = list(matrix.index.tolist())

ucb=nx.Graph()

ucb.add_nodes_from(bipart_0, bipartite=0)
ucb.add_nodes_from(bipart_1, bipartite=1)

for i in matrix.columns:
    edge_list = [edge for edge in matrix[i].tolist() if math.isnan(edge[2]) == False]
    if len(edge_list) > 0:
        ucb.add_weighted_edges_from(edge_list, weight='avg rating')

"""Using user_category BiPartite"""

def get_jaccard_mean_scores(score_list):
#     try:
#         return mode(score_list)
#     except:
#         return statistics.mean(score_list)
    return statistics.mean(score_list)

"""Line 3 of the first for loop is the adamic_adar_index; we can change this to another metric if we want"""

train['graph_tuple'] = list(zip(train.user_id, train.business_id, train.stars))

train_user_ids = train.user_id.tolist()
train_business_ids = train.business_id.tolist()
test['user_business'] = list(zip(test.user_id, test.business_id))

B=nx.Graph()

bipart_0 = list(train.user_id)
bipart_1 = list(train.business_id)

B.add_nodes_from(bipart_0, bipartite=0)
B.add_nodes_from(bipart_1, bipartite=1)

B.add_weighted_edges_from(train.graph_tuple, weight='avg rating')

similarity_list = []

for user, business in test.user_business.tolist():
    ego_network = nx.ego_graph(B, n=business)
    jaccard_list = [(i,user) for i in ego_network.nodes if i != business]
    preds = nx.adamic_adar_index(ucb, jaccard_list)
    highest_similarity_score = 0
    highest_similarity_nodes = []
    for u,v,p in preds:
        if p >= highest_similarity_score:
            if p > highest_similarity_score:
                highest_similarity_nodes = []
                highest_similarity_nodes.append(u)
                highest_similarity_score = p
            else:
                highest_similarity_nodes.append(u)
    similarity_list.append([user,business,highest_similarity_nodes,highest_similarity_score])
    
found_jaccard_score = pd.DataFrame(data = similarity_list, columns=['user_id', 'business_id', 'found_neighbors','jaccard_score'])

found_jaccard_score['neighbor_ratings'] = found_jaccard_score\
            .apply(lambda row:get_jaccard_neighbor_scores(row.business_id, row.found_neighbors) if type(row.found_neighbors) == list else None ,axis=1)

found_jaccard_score['projected_stars'] = found_jaccard_score.neighbor_ratings.apply(lambda x: get_jaccard_mean_scores(x) if type(x) == list else None)

test = test[['user_id','business_id','stars']]

found_jaccard_score = found_jaccard_score.merge(test, on=['user_id','business_id'])

mean_squared_error(found_jaccard_score.stars, found_jaccard_score.projected_stars)

"""# RMSE evaluation of baseline predictions
baseline is average rating of each establishment
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt

# Loading reviews.csv
reviews = pd.read_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/reviews.csv')
reviews.head()

# Re order columns
reviews = reviews[['user_id', 'business_id', 'stars']]

# Number of reviews
reviews.shape[0]

# # Reviews is the dataframe containing user_id, business_id, and the true rating they left
# # Split this into training and test
# x = reviews[['user_id', 'business_id']]
# y = reviews['stars']

# # Default 75% / 25% split
# X_train, X_test, Y_train, Y_test = train_test_split(x, y)

# # Save training set
# # with open('X_train.csv', 'w') as f:
# #   for i,r in X_train.iterrows(): 
# #     f.write(str(r[0]) + ',' + str(r[1]) + '\n')
# X_train.to_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/X_train.csv')

# # Save test set
# # with open('X_test.csv', 'w') as f:
# #   for i,r in X_train.iterrows(): 
# #     f.write(str(r[0]) + ',' + str(r[1]) + '\n')
# X_test.to_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/X_test.csv')

# # Save training ratings
# # with open('Y_train.csv', 'w') as f:
# #   for i,r in Y_train.reset_index().iterrows(): 
# #     f.write(str(r[0]) + ',' + str(r[1]) + '\n')
# Y_train.to_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/Y_train.csv')

# # Save training ratings
# # with open('Y_test.csv', 'w') as f:
# #   for i,r in Y_test.reset_index().iterrows(): 
# #     f.write(str(r[0]) + ',' + str(r[1]) + '\n')
# Y_test.to_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/Y_test.csv')

# train=pd.read_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/train.csv')
test=pd.read_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/equal_distribution_test.csv')

test.head()

yelp_business=pd.read_json('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/yelp_academic_dataset_business.json', lines=True)

##Loading in the user_dataset
data=[]
yelp_user=pd.read_json('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/yelp_academic_dataset_user.json',lines=True,chunksize=100000)

#Loading the json reader together into a data list
for i in yelp_user:
  data.append(i)
#Concatenating the chunks together to load the entire dataset
yelp_user=pd.concat(data)

# buisness ave rating as baseline predictor
business_baseline = test.merge(yelp_business, how='left', on='business_id', sort=False)
business_baseline.head()

def rms_test(true_data, predicted_data):
    '''Calculate the RMSE value of predicted values against true values
    
    :params true_data: list of true values
    :type true_data: list
    :params predicted_data: list of predicted values
    :type predicted_data: list
    :returns: Root Mean Square Error Value
    :rtype: float
    '''
   
    
    return sqrt(mean_squared_error(true_data, predicted_data))

# RMSE of baseline - Average business rating
rms_test(test.stars.tolist(), business_baseline.stars_y.tolist())

# User ave rating as baseline predictor
user_baseline = test.merge(test.merge(yelp_user, how='left', on='user_id', sort=False))
user_baseline.head()

# RMSE of baseline - Average user rating
rms_test(test.stars.tolist(), user_baseline.average_stars)

# Making edgelist for test data
test_bi_list = list(zip(X_test['business_id'], X_test['user_id'], Y_test[0]))

# Making bipartite graph of test data
test_graph=nx.Graph()

test_graph.add_nodes_from(X_test['business_id'], bipartite=0)
test_graph.add_nodes_from(X_test['user_id'], bipartite=1)
test_graph.add_weighted_edges_from(test_bi_list, weight='stars')

# Function for finding rating of highest rated user by pagerank
def highest_page(business):
  ego=nx.ego_graph(test_graph, business, radius=1)
  pr=nx.pagerank(ego,weight='stars')
  highest = sorted(pr.items(), reverse=True, key=lambda kv: kv[1])[1][0]
#   highest=sorted_pr[1][0]
#   highest_star=reviews.loc[(reviews['user_id']==highest) & (reviews['business_id']==business),'stars']
  return (business, highest)

# # %%time
# highest_page('mUVAMNN7BCQ9HGA9w_7C1g')
# (len(X_test)*69)/1000/60/60

# page_rank_baseline=[highest_page(x) for x in X_test['business_id']]

# user_category=pd.read_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/user_category_tuples.csv')
# user_category.columns

"""# One-mode Projection"""

import pandas as pd
import numpy as np
import networkx as nx

reviews_filtered=pd.read_csv('gdrive/My Drive/SI_608_Group_Project/yelp_dataset/reviews_filtered.csv')

# Making edgelist for reviews_filtered data
test_bi_list = list(zip(reviews_filtered['business_id'], reviews_filtered['user_id'], reviews_filtered['stars']))

# Making bipartite graph of reviews_filtered data
test_graph=nx.Graph()

test_graph.add_nodes_from(reviews_filtered['business_id'], bipartite=0)
test_graph.add_nodes_from(reviews_filtered['user_id'], bipartite=1)
test_graph.add_weighted_edges_from(test_bi_list, weight='stars')

from networkx.algorithms import bipartite
one_mode=bipartite.weighted_projected_graph(test_graph,reviews_filtered['user_id'].unique() )

# makes largest connected component for one-mode projection
largest_connected_comp=max(nx.connected_component_subgraphs(one_mode),key=len)

#Distance Based Methods
#Now for every nodes in the bipartite network let's do a neighbor search 

#Now Query the bi-partite graph neighbors of the current business
ratings=[]
for x,i in test_set.iterrows():
    a={}
    neighbors=test_graph.neighbors(i['business_id'])
    
    a={k:len(nx.shortest_path(one_mode,source=k,target=i['user_id'])) for k in neighbors if k!=i['user_id']}
    sorted_by_value = sorted(a.items(), key=lambda kv: kv[1],reverse=True)
    b=[x[0] for x in sorted_by_value[0:5]]
    j=0
            
    ratings.append(np.average(train_set[(train_set['user_id'].isin(b))&(train_set['business_id']==i['business_id'])]['stars']))

#Eigen Vector Centrality
ratings=[]
for x,i in test_set.iterrows():
    a={}
    neighbors=test_graph.neighbors(i['business_id'])
    
    G=one_mode.subgraph(neighbors)
    a={x:K[x] for x in G.nodes()}
    sorted_by_value = sorted(a.items(), key=lambda kv: kv[1])
    b=[x[0] for x in sorted_by_value[0:10]]
   
            
    ratings.append(np.average(train_set[(train_set['user_id'].isin(b))&(train_set['business_id']==i['business_id'])]['stars']))